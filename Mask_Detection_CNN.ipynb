{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DR-eO17geWu"
   },
   "source": [
    "# Mask detection model with CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EMefrVPCg-60"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCV30xyVhFbE",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os, split_folders\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxQxCBWyoGPE"
   },
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvE-heJNo3GG"
   },
   "source": [
    "### Preprocessing the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 1376 files [02:11, 10.44 files/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate training set and test set\n",
    "data_path = 'dataset'\n",
    "split_folders.ratio('dataset', output=\"datasets\", seed=1159, ratio=(.8, .2)) # default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1100 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "img_size =(100,100)\n",
    "# apply data augmentation\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   rotation_range=20,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   horizontal_flip = False)\n",
    "training_set = train_datagen.flow_from_directory('datasets/train',\n",
    "                                                 target_size = img_size,\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrCMmGw9pHys"
   },
   "source": [
    "### Preprocessing the test set\n",
    "\n",
    "The image dataset was prepared by [Prajna Bhandary](https://github.com/prajnasb/observations/tree/master/experiements/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SH4WzfOhpKc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 276 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory('datasets/val',\n",
    "                                            target_size = img_size,\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "af8O4l90gk7B"
   },
   "source": [
    "## Part 2 - Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ces1gXY2lmoX"
   },
   "source": [
    "### Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SAUt4UMPlhLS"
   },
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5YJj_XMl5LF"
   },
   "source": [
    "### Step 1 - Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XPzPrMckl-hV"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=200, kernel_size=3, activation='relu', input_shape=[100, 100, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tf87FpvxmNOJ"
   },
   "source": [
    "### Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ncpqPl69mOac"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xaTOgD8rm4mU"
   },
   "source": [
    "### Adding a second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i_-FZjn_m8gk"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=100, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tmiEuvTunKfk"
   },
   "source": [
    "### Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6AZeOGCvnNZn"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dAoSECOm203v"
   },
   "source": [
    "### Step 4 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8GtmUlLd26Nq"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=64, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yTldFvbX28Na"
   },
   "source": [
    "### Step 5 - Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1p_Zj1Mc3Ko_"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D6XkI90snSDl"
   },
   "source": [
    "## Part 3 - Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfrFQACEnc6i"
   },
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NALksrNQpUlJ"
   },
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehS-v3MIpX2h"
   },
   "source": [
    "### Training the CNN on the training set and evaluating it on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUj1W4PJptta",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "35/35 [==============================] - 168s 5s/step - loss: 0.6422 - accuracy: 0.6445 - val_loss: 0.3435 - val_accuracy: 0.9130\n",
      "Epoch 2/20\n",
      "35/35 [==============================] - 171s 5s/step - loss: 0.3698 - accuracy: 0.8591 - val_loss: 0.2405 - val_accuracy: 0.9058\n",
      "Epoch 3/20\n",
      "35/35 [==============================] - 115s 3s/step - loss: 0.3084 - accuracy: 0.8945 - val_loss: 0.2701 - val_accuracy: 0.9022\n",
      "Epoch 4/20\n",
      "35/35 [==============================] - 148s 4s/step - loss: 0.2809 - accuracy: 0.8973 - val_loss: 0.1627 - val_accuracy: 0.9565\n",
      "Epoch 5/20\n",
      "35/35 [==============================] - 115s 3s/step - loss: 0.2240 - accuracy: 0.9182 - val_loss: 0.1766 - val_accuracy: 0.9529\n",
      "Epoch 6/20\n",
      "35/35 [==============================] - 97s 3s/step - loss: 0.2187 - accuracy: 0.9191 - val_loss: 0.2308 - val_accuracy: 0.9420\n",
      "Epoch 7/20\n",
      "35/35 [==============================] - 91s 3s/step - loss: 0.2367 - accuracy: 0.9118 - val_loss: 0.1578 - val_accuracy: 0.9565\n",
      "Epoch 8/20\n",
      "35/35 [==============================] - 91s 3s/step - loss: 0.1859 - accuracy: 0.9364 - val_loss: 0.1112 - val_accuracy: 0.9638\n",
      "Epoch 9/20\n",
      "35/35 [==============================] - 91s 3s/step - loss: 0.1419 - accuracy: 0.9473 - val_loss: 0.1332 - val_accuracy: 0.9565\n",
      "Epoch 10/20\n",
      "35/35 [==============================] - 91s 3s/step - loss: 0.1348 - accuracy: 0.9527 - val_loss: 0.1148 - val_accuracy: 0.9674\n",
      "Epoch 11/20\n",
      "35/35 [==============================] - 132s 4s/step - loss: 0.1171 - accuracy: 0.9573 - val_loss: 0.0860 - val_accuracy: 0.9674\n",
      "Epoch 12/20\n",
      "35/35 [==============================] - 94s 3s/step - loss: 0.1586 - accuracy: 0.9445 - val_loss: 0.1198 - val_accuracy: 0.9601\n",
      "Epoch 13/20\n",
      "35/35 [==============================] - 104s 3s/step - loss: 0.1035 - accuracy: 0.9591 - val_loss: 0.1283 - val_accuracy: 0.9638\n",
      "Epoch 14/20\n",
      "35/35 [==============================] - 96s 3s/step - loss: 0.1449 - accuracy: 0.9464 - val_loss: 0.1180 - val_accuracy: 0.9565\n",
      "Epoch 15/20\n",
      "35/35 [==============================] - 92s 3s/step - loss: 0.1221 - accuracy: 0.9600 - val_loss: 0.0847 - val_accuracy: 0.9674\n",
      "Epoch 16/20\n",
      "35/35 [==============================] - 90s 3s/step - loss: 0.1110 - accuracy: 0.9582 - val_loss: 0.0796 - val_accuracy: 0.9783\n",
      "Epoch 17/20\n",
      "35/35 [==============================] - 94s 3s/step - loss: 0.1144 - accuracy: 0.9636 - val_loss: 0.0791 - val_accuracy: 0.9746\n",
      "Epoch 18/20\n",
      "35/35 [==============================] - 90s 3s/step - loss: 0.1030 - accuracy: 0.9627 - val_loss: 0.1585 - val_accuracy: 0.9601\n",
      "Epoch 19/20\n",
      "35/35 [==============================] - 92s 3s/step - loss: 0.1136 - accuracy: 0.9591 - val_loss: 0.1421 - val_accuracy: 0.9601\n",
      "Epoch 20/20\n",
      "35/35 [==============================] - 92s 3s/step - loss: 0.0997 - accuracy: 0.9727 - val_loss: 0.1613 - val_accuracy: 0.9420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ba9d477408>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3PZasO0006Z"
   },
   "source": [
    "## Part 4 - Predicting face mask in real-time with OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the face classifier and initiate camera\n",
    "face_recog=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "source=cv2.VideoCapture(0)\n",
    "# Define label and color dictionaries\n",
    "labels_dic={0:'WITH MASK',1:'WITHOUT MASK'}\n",
    "color_dic={0:(0,255,0),1:(0,0,255)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while(True):\n",
    "\n",
    "    ret,img=source.read()\n",
    "    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces=face_recog.detectMultiScale(gray,1.3,5)  \n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "    \n",
    "        face_img=img[y:y+w,x:x+w]\n",
    "        resized=cv2.resize(face_img,img_size) # resize the face image\n",
    "        normalized=resized/255.0 # normalize the image\n",
    "        img_exp=np.expand_dims(normalized,axis=0)\n",
    "        result=cnn.predict(img_exp)  # make prediction\n",
    "        label= np.argmax(result,axis=1)[0]\n",
    "\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),color_dic[label],2)\n",
    "        cv2.rectangle(img,(x,y-40),(x+w,y),color_dic[label],-1)\n",
    "        mask_label = \"{}: {:.2f}%\".format(labels_dic[label], round(np.max(result,axis=1)[0]*100,2))\n",
    "        cv2.putText(img, mask_label, (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.6,(255,255,255),2) \n",
    "        \n",
    "    cv2.imshow('LIVE',img)\n",
    "    key=cv2.waitKey(1)\n",
    "    \n",
    "    if(key==27):\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "source.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ED9KB3I54c1i"
   },
   "source": [
    "This model was motivated by the work of [The Perceptron](https://github.com/aieml/face-mask-detection-keras).\n",
    "The image dataset was prepared by [Prajna Bhandary](https://github.com/prajnasb/observations/tree/master/experiements/data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "convolutional_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
